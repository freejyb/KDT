{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "336979bd-7e72-40b2-be44-9132f060bc33",
   "metadata": {},
   "source": [
    "### 파이프라인(Pipeline) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1eb92-0a1a-4833-b84b-5ecdadfe5e9d",
   "metadata": {},
   "source": [
    "- 분류(Classification) 분석을 진행할 때 파이프라인(Pipeline)을 활용하면 \n",
    "- 전처리 → 특징 변환 → 모델 학습 → 평가 과정을 깔끔하게 묶어서 처리할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b190a3-97a1-4eed-bc51-e3c7a26b7b1e",
   "metadata": {},
   "source": [
    "## 1. 기본 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee950a85-344f-4c58-bbc1-725bf01aef47",
   "metadata": {},
   "source": [
    "### 1-1 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df79e43-693d-4cb0-b9e2-7ff62e78c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 로드\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# 학습용/테스트용 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de39373c-735d-4d9e-923e-6158297542f2",
   "metadata": {},
   "source": [
    "### 1-2. 파이프라인 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d844826-b979-48a2-a937-eba7d08d845e",
   "metadata": {},
   "source": [
    "- 분류모델 (로지스틱회귀)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939dd142-cdd9-4f41-86b9-ea81b17f3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),   # 결측치 처리\n",
    "    (\"scaler\", StandardScaler()),                  # 스케일링\n",
    "    (\"model\", LogisticRegression(max_iter=1000))   # 분류 모델\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f53f83-00c9-4f30-a468-b3a7ba4a072b",
   "metadata": {},
   "source": [
    "### 1-3. 모델 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bca90a7-2eeb-4ec7-91e9-b8c8fc770402",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711ebe4-ae8f-450a-8d2b-7a89d334b7e6",
   "metadata": {},
   "source": [
    "### 1-4. 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e40e33-fdbe-4c2d-b7de-8c3df9d7521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0]\n",
      " [ 0 14  0]\n",
      " [ 0  0  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca35362-5720-485e-97e7-36fd8a282a64",
   "metadata": {},
   "source": [
    "### 1-5.파이프라인 + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e069cc5-d719-4576-9945-d3ea2c4abda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 내부 모델 파라미터는 \"단계이름__파라미터명\" 형식으로 접근\n",
    "\n",
    "# 전처리 하이퍼파라미터(예: 스케일링 방식)도 동일한 방식으로 튜닝 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42246118-6929-4233-9ede-f2864ea63013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 파라미터: {'model__C': 1, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
      "최적 점수: 0.9859605911330049\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 탐색할 하이퍼파라미터 정의\n",
    "param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10],\n",
    "    \"model__penalty\": [\"l1\", \"l2\"],\n",
    "    \"model__solver\": [\"liblinear\"]\n",
    "}\n",
    "\n",
    "# 그리드서치 객체 생성\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring=\"accuracy\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"최적 파라미터:\", grid.best_params_)\n",
    "print(\"최적 점수:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888b52f-d70d-44c0-aad4-068249e4fabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b35daea-542e-4b73-a5b3-162207605bc1",
   "metadata": {},
   "source": [
    "### 1-6. ColumnTransformer와 함께 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76089b2d-07d8-4a61-a568-c26b1c7ded3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실무에서는 수치형 + 범주형 변수가 섞여 있는 경우--> 이럴 땐 ColumnTransformer를 파이프라인에 통합하면 편리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc095c37-d300-4f71-ba97-cb0fb92fc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 예시: 0~2번 컬럼은 수치형, 3~4번 컬럼은 범주형이라고 가정\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), [0, 1, 2]),\n",
    "    (\"cat\", OneHotEncoder(), [3, 4])\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566e6fb-9371-4d27-9a57-ce7c88bd4be4",
   "metadata": {},
   "source": [
    "## 2. LogisticRegression / RandomForest / XGBoost / SVM 네 가지 분류기를 각각 GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff68032-34ac-4ad1-b4d1-6a875822bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer:\n",
    "# 수치형(num_*) → SimpleImputer(median) → StandardScaler\n",
    "# 범주형(cat_0, cat_1) → SimpleImputer(most_frequent) → OneHotEncoder(handle_unknown=\"ignore\")\n",
    "# Pipeline (\"preprocess\", preprocessor) → (\"model\", <Classifier>)\n",
    "# GridSearchCV: 전처리 결과를 훈련 폴드에서만 학습에 사용하므로 데이터 누수 방지\n",
    "# 공통 cv=StratifiedKFold(5), scoring=\"f1_macro\"\n",
    "# 평가: Accuracy, F1(macro), Confusion Matrix, Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef203549-0dcb-4d6a-ab34-195968389892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "[LogisticRegression]\n",
      "Best params: {'model__C': 0.1, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n",
      "CV Best F1(macro): 0.7904\n",
      "Test Accuracy   : 0.8125\n",
      "Test F1 (macro) : 0.7945\n",
      "\n",
      "Confusion Matrix:\n",
      " [[133  11]\n",
      " [ 34  62]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7964    0.9236    0.8553       144\n",
      "           1     0.8493    0.6458    0.7337        96\n",
      "\n",
      "    accuracy                         0.8125       240\n",
      "   macro avg     0.8229    0.7847    0.7945       240\n",
      "weighted avg     0.8176    0.8125    0.8067       240\n",
      "\n",
      "======================================================================\n",
      "[RandomForest]\n",
      "Best params: {'model__max_depth': None, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 400}\n",
      "CV Best F1(macro): 0.9206\n",
      "Test Accuracy   : 0.9250\n",
      "Test F1 (macro) : 0.9203\n",
      "\n",
      "Confusion Matrix:\n",
      " [[140   4]\n",
      " [ 14  82]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9091    0.9722    0.9396       144\n",
      "           1     0.9535    0.8542    0.9011        96\n",
      "\n",
      "    accuracy                         0.9250       240\n",
      "   macro avg     0.9313    0.9132    0.9203       240\n",
      "weighted avg     0.9268    0.9250    0.9242       240\n",
      "\n",
      "======================================================================\n",
      "[XGBoost]\n",
      "Best params: {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.05, 'model__max_depth': 5, 'model__subsample': 0.8}\n",
      "CV Best F1(macro): 0.9337\n",
      "Test Accuracy   : 0.9375\n",
      "Test F1 (macro) : 0.9345\n",
      "\n",
      "Confusion Matrix:\n",
      " [[138   6]\n",
      " [  9  87]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9388    0.9583    0.9485       144\n",
      "           1     0.9355    0.9062    0.9206        96\n",
      "\n",
      "    accuracy                         0.9375       240\n",
      "   macro avg     0.9371    0.9323    0.9345       240\n",
      "weighted avg     0.9375    0.9375    0.9373       240\n",
      "\n",
      "======================================================================\n",
      "[SVM]\n",
      "Best params: {'model__C': 2.0, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\n",
      "CV Best F1(macro): 0.9088\n",
      "Test Accuracy   : 0.9375\n",
      "Test F1 (macro) : 0.9338\n",
      "\n",
      "Confusion Matrix:\n",
      " [[141   3]\n",
      " [ 12  84]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9216    0.9792    0.9495       144\n",
      "           1     0.9655    0.8750    0.9180        96\n",
      "\n",
      "    accuracy                         0.9375       240\n",
      "   macro avg     0.9435    0.9271    0.9338       240\n",
      "weighted avg     0.9391    0.9375    0.9369       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 분류 파이프라인 4종 비교 스크립트\n",
    "# Logistic / RandomForest / XGBoost / SVM\n",
    "# ===============================\n",
    "\n",
    "# 1. 라이브러리 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# XGBoost (pip install xgboost)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# 2. 데이터 생성: 수치형 + 범주형 혼합 (자급자족) \n",
    "# 분류(Classification)용 샘플 데이터 생성 함수\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(42)  # 난수 생성기의 **시드(seed)**를 고정하는 역할\n",
    "X_num, y = make_classification(\n",
    "    n_samples=1200,     # 총 샘플 수 (1200개의 데이터 포인트 생성)\n",
    "    n_features=6,    # 총 샘플 수 (1200개의 데이터 포인트 생성)\n",
    "    n_informative=4,  # 실제로 분류에 기여하는 중요한 특징 수\n",
    "    n_redundant=0,   # informative 특징을 선형결합해서 만든 중복 특징 수\n",
    "    n_repeated=0,  # informative/redundant 특징을 그대로 복사한 특징 수\n",
    "    n_classes=2,  # 클래스 개수 → 이진 분류\n",
    "    class_sep=1.2, # 클래스 간 중심 간격 → 값이 클수록 클래스가 더 잘 분리됨\n",
    "    weights=[0.6, 0.4],   # 클래스별 샘플 비율 지정  → 클래스 0: 60%, 클래스 1: 40%\n",
    "    random_state=42 # 난수 시드 → 항상 동일한 데이터셋 생성\n",
    ")\n",
    "\n",
    "\n",
    "# NumPy 배열을 pandas DataFrame으로 변환하면서, 자동으로 컬럼명까지 생성하는 부분 : 수치형 6개를 DataFrame으로\n",
    "# X_num : NumPy 배열 → shape = (1200, 6)\n",
    "# → make_classification()으로 생성한 1200개 샘플 × 6개 특징 데이터\n",
    "# pd.DataFrame() : NumPy 배열을 pandas DataFrame으로 변환\n",
    "# columns=[...] : 컬럼명을 자동 생성하여 지정\n",
    "# X_num.shape[1] → X_num의 열 개수, 즉 특징(feature) 수\n",
    "\n",
    "df = pd.DataFrame(X_num, columns=[f\"num_{i}\" for i in range(X_num.shape[1])])\n",
    "\n",
    "# 범주형 특성 2개를 추가 (의미 있는 신호 약간 부여)\n",
    "# cat_0: 3개 레벨(A/B/C), y=1일 때 B 비율 조금 높게\n",
    "cat0 = np.where(np.random.rand(len(y)) + 0.15*(y==1) > 0.67, \"B\",\n",
    "        np.where(np.random.rand(len(y)) > 0.5, \"A\", \"C\"))\n",
    "\n",
    "# cat_1: 4개 레벨, 무작위\n",
    "cat1_levels = np.array([\"X\",\"Y\",\"Z\",\"W\"])\n",
    "cat1 = np.random.choice(cat1_levels, size=len(y), p=[0.25,0.25,0.25,0.25])\n",
    "\n",
    "df[\"cat_0\"] = cat0\n",
    "df[\"cat_1\"] = cat1\n",
    "\n",
    "# 목표변수\n",
    "target = pd.Series(y, name=\"target\")\n",
    "\n",
    "# 학습/검증 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, target, test_size=0.2, random_state=42, stratify=target\n",
    ")\n",
    "\n",
    "# 3. 전처리 파이프라인 구성 -----\n",
    "num_cols = [c for c in df.columns if c.startswith(\"num_\")]\n",
    "cat_cols = [\"cat_0\", \"cat_1\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# ColumnTransformer를 사용해 수치형(numeric)과 범주형(categorical) 데이터를 각각 다른 전처리 방식으로 처리하기 위한 설정\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. 네 가지 모델용 파이프라인 정의 -----\n",
    "pipe_logit = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=2000))\n",
    "])\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        tree_method=\"hist\",      # CPU 빠른 학습\n",
    "        random_state=42,\n",
    "        n_estimators=300\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# 5. 하이퍼파라미터 그리드 -----\n",
    "param_logit = {\n",
    "    \"model__C\": [0.1, 1.0, 10.0],\n",
    "    \"model__penalty\": [\"l2\"],          # (liblinear일 경우 l1도 가능)\n",
    "    \"model__solver\": [\"lbfgs\"]         # 다수 특성에 안정적\n",
    "}\n",
    "\n",
    "param_rf = {\n",
    "    \"model__n_estimators\": [200, 400],\n",
    "    \"model__max_depth\": [None, 8, 12],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2]\n",
    "}\n",
    "\n",
    "param_xgb = {\n",
    "    \"model__max_depth\": [3, 5],\n",
    "    \"model__learning_rate\": [0.05, 0.1],\n",
    "    \"model__subsample\": [0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_svm = {\n",
    "    \"model__C\": [0.5, 1.0, 2.0],\n",
    "    \"model__kernel\": [\"rbf\", \"poly\"],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": (pipe_logit, param_logit),\n",
    "    \"RandomForest\": (pipe_rf, param_rf),\n",
    "    \"XGBoost\": (pipe_xgb, param_xgb),\n",
    "    \"SVM\": (pipe_svm, param_svm)\n",
    "}\n",
    "\n",
    "# 6. 공통 CV 설정\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def run_grid_search(name, pipe, param_grid):\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        scoring=\"f1_macro\",      # 불균형 대비 종합성\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best = grid.best_estimator_\n",
    "    y_pred = best.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1m = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(f\"[{name}]\")\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "    print(f\"CV Best F1(macro): {grid.best_score_:.4f}\")\n",
    "    print(f\"Test Accuracy   : {acc:.4f}\")\n",
    "    print(f\"Test F1 (macro) : {f1m:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# 7. 실행 -----\n",
    "for name, (pipe, params) in models.items():\n",
    "    run_grid_search(name, pipe, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e86386-7fdd-4162-be1b-3d012efbba95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34dce029-532d-44ac-b696-9fb5727ec2e7",
   "metadata": {},
   "source": [
    "- ColumnTransformer + Pipeline을 공통 전처리로 묶고, \n",
    "- LogisticRegression / RandomForest / XGBoost / SVM 네 모델을 GridSearchCV로 튜닝·평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8207f006-6be4-408a-8880-bc1f4252e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "[LogisticRegression]\n",
      "Best params: {'model__C': 1.0, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n",
      "CV Best f1_macro: 0.7885\n",
      "Test Accuracy   : 0.8125\n",
      "Test F1 (macro) : 0.7967\n",
      "\n",
      "Confusion Matrix:\n",
      " [[131  13]\n",
      " [ 32  64]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8037    0.9097    0.8534       144\n",
      "           1     0.8312    0.6667    0.7399        96\n",
      "\n",
      "    accuracy                         0.8125       240\n",
      "   macro avg     0.8174    0.7882    0.7967       240\n",
      "weighted avg     0.8147    0.8125    0.8080       240\n",
      "\n",
      "========================================================================\n",
      "[RandomForest]\n",
      "Best params: {'model__max_depth': None, 'model__min_samples_leaf': 1, 'model__min_samples_split': 5, 'model__n_estimators': 400}\n",
      "CV Best f1_macro: 0.9204\n",
      "Test Accuracy   : 0.9167\n",
      "Test F1 (macro) : 0.9111\n",
      "\n",
      "Confusion Matrix:\n",
      " [[140   4]\n",
      " [ 16  80]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8974    0.9722    0.9333       144\n",
      "           1     0.9524    0.8333    0.8889        96\n",
      "\n",
      "    accuracy                         0.9167       240\n",
      "   macro avg     0.9249    0.9028    0.9111       240\n",
      "weighted avg     0.9194    0.9167    0.9156       240\n",
      "\n",
      "========================================================================\n",
      "[SVM]\n",
      "Best params: {'model__C': 2.0, 'model__gamma': 'scale', 'model__kernel': 'rbf'}\n",
      "CV Best f1_macro: 0.9193\n",
      "Test Accuracy   : 0.9333\n",
      "Test F1 (macro) : 0.9298\n",
      "\n",
      "Confusion Matrix:\n",
      " [[139   5]\n",
      " [ 11  85]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9267    0.9653    0.9456       144\n",
      "           1     0.9444    0.8854    0.9140        96\n",
      "\n",
      "    accuracy                         0.9333       240\n",
      "   macro avg     0.9356    0.9253    0.9298       240\n",
      "weighted avg     0.9338    0.9333    0.9329       240\n",
      "\n",
      "========================================================================\n",
      "[XGBoost]\n",
      "Best params: {'model__colsample_bytree': 1.0, 'model__learning_rate': 0.1, 'model__max_depth': 5, 'model__n_estimators': 200, 'model__subsample': 1.0}\n",
      "CV Best f1_macro: 0.9368\n",
      "Test Accuracy   : 0.9458\n",
      "Test F1 (macro) : 0.9433\n",
      "\n",
      "Confusion Matrix:\n",
      " [[139   5]\n",
      " [  8  88]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9456    0.9653    0.9553       144\n",
      "           1     0.9462    0.9167    0.9312        96\n",
      "\n",
      "    accuracy                         0.9458       240\n",
      "   macro avg     0.9459    0.9410    0.9433       240\n",
      "weighted avg     0.9458    0.9458    0.9457       240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 분류 파이프라인 4종 비교 (ColumnTransformer + Pipeline + GridSearchCV)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# (선택) XGBoost: 미설치면 자동 건너뜀\n",
    "has_xgb = True\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except Exception:\n",
    "    has_xgb = False\n",
    "\n",
    "# 1) ----- 데이터 만들기: 수치형 6개 + 범주형 2개 -----\n",
    "from sklearn.datasets import make_classification\n",
    "np.random.seed(42)\n",
    "X_num, y = make_classification(\n",
    "    n_samples=1200, n_features=6, n_informative=4, n_redundant=0,\n",
    "    n_repeated=0, n_classes=2, class_sep=1.2, weights=[0.6, 0.4],\n",
    "    random_state=42\n",
    ")\n",
    "df = pd.DataFrame(X_num, columns=[f\"num_{i}\" for i in range(X_num.shape[1])])\n",
    "\n",
    "# 범주형 특성 2개 (간단 신호/노이즈)\n",
    "cat0 = np.where(np.random.rand(len(y)) + 0.15*(y==1) > 0.67, \"B\",\n",
    "        np.where(np.random.rand(len(y)) > 0.5, \"A\", \"C\"))\n",
    "cat1 = np.random.choice([\"X\",\"Y\",\"Z\",\"W\"], size=len(y))\n",
    "df[\"cat_0\"] = cat0\n",
    "df[\"cat_1\"] = cat1\n",
    "target = pd.Series(y, name=\"target\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, target, test_size=0.2, stratify=target, random_state=42\n",
    ")\n",
    "\n",
    "# 2) ----- 전처리: 수치형/범주형 분리 처리 -----\n",
    "num_cols = [c for c in df.columns if c.startswith(\"num_\")]\n",
    "cat_cols = [\"cat_0\", \"cat_1\"]\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3) ----- 모델 파이프라인들 -----\n",
    "pipe_logit = Pipeline([(\"preprocess\", preprocessor),\n",
    "                       (\"model\", LogisticRegression(max_iter=2000))])\n",
    "\n",
    "pipe_rf    = Pipeline([(\"preprocess\", preprocessor),\n",
    "                       (\"model\", RandomForestClassifier(random_state=42))])\n",
    "\n",
    "pipe_svm   = Pipeline([(\"preprocess\", preprocessor),\n",
    "                       (\"model\", SVC(probability=True, random_state=42))])\n",
    "\n",
    "if has_xgb:\n",
    "    pipe_xgb = Pipeline([(\"preprocess\", preprocessor),\n",
    "                         (\"model\", XGBClassifier(\n",
    "                             objective=\"binary:logistic\",\n",
    "                             eval_metric=\"logloss\",\n",
    "                             tree_method=\"hist\",\n",
    "                             random_state=42,\n",
    "                         ))])\n",
    "\n",
    "# 4) ----- 하이퍼파라미터 그리드 -----\n",
    "param_logit = {\n",
    "    \"model__C\": [0.1, 1.0, 10.0],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "    \"model__solver\": [\"lbfgs\"],\n",
    "}\n",
    "param_rf = {\n",
    "    \"model__n_estimators\": [200, 400],\n",
    "    \"model__max_depth\": [None, 10],\n",
    "    \"model__min_samples_split\": [2, 5],\n",
    "    \"model__min_samples_leaf\": [1, 2],\n",
    "}\n",
    "param_svm = {\n",
    "    \"model__C\": [0.5, 1.0, 2.0],\n",
    "    \"model__kernel\": [\"rbf\", \"poly\"],\n",
    "    \"model__gamma\": [\"scale\", \"auto\"],\n",
    "}\n",
    "if has_xgb:\n",
    "    param_xgb = {\n",
    "        \"model__max_depth\": [3, 5],\n",
    "        \"model__learning_rate\": [0.05, 0.1],\n",
    "        \"model__subsample\": [0.8, 1.0],\n",
    "        \"model__colsample_bytree\": [0.8, 1.0],\n",
    "        \"model__n_estimators\": [200, 400],\n",
    "    }\n",
    "\n",
    "models = [\n",
    "    (\"LogisticRegression\", pipe_logit, param_logit),\n",
    "    (\"RandomForest\",       pipe_rf,   param_rf),\n",
    "    (\"SVM\",                pipe_svm,  param_svm),\n",
    "]\n",
    "if has_xgb:\n",
    "    models.append((\"XGBoost\", pipe_xgb, param_xgb))\n",
    "\n",
    "# 5) ----- 공통 CV/스코어 -----\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "SCORING = \"f1_macro\"  # 불균형 대응 종합성\n",
    "\n",
    "def run_grid(name, pipe, grid):\n",
    "    gs = GridSearchCV(pipe, grid, cv=cv, scoring=SCORING, n_jobs=-1, verbose=0)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best = gs.best_estimator_\n",
    "    y_pred = best.predict(X_test)\n",
    "    acc  = accuracy_score(y_test, y_pred)\n",
    "    f1m  = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    print(\"=\"*72)\n",
    "    print(f\"[{name}]\")\n",
    "    print(\"Best params:\", gs.best_params_)\n",
    "    print(f\"CV Best {SCORING}: {gs.best_score_:.4f}\")\n",
    "    print(f\"Test Accuracy   : {acc:.4f}\")\n",
    "    print(f\"Test F1 (macro) : {f1m:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "for name, pipe, grid in models:\n",
    "    run_grid(name, pipe, grid)\n",
    "\n",
    "if not has_xgb:\n",
    "    print(\"\\n[알림] xgboost 미설치로 XGBoost 실험은 건너뛰었습니다. (pip install xgboost)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71b4055-769c-481f-a484-7a36f4f21f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
